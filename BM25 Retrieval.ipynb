{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Imports\n",
    "The following imports are required to run the code in the notebook, look at the comments for any necessary explanation on the package."
   ],
   "id": "b8debe528e1e8ebc"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-15T16:22:33.853090Z",
     "start_time": "2025-12-15T16:22:33.850013Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Imports\n",
    "import json # JSON Package ( Used for opening datasets, corpus )\n",
    "import bm25s # The BM25 Sparse Retrieval Implementation ( Used for Retrieval of Doc Scores )\n",
    "import nltk  #  Natural Language ToolKit ( Used for Tokenization )\n",
    "from tqdm import tqdm # Progress Bar ( Used for Showing Progress of Functions )\n",
    "import numpy as np # Scientific Computing ( Used for Mathematical Functions )\n",
    "\n",
    "# Download Natural Language Toolkit tokenizer model(s) for word_tokenizer()\n",
    "# NOTE : Change to 'quiet=False'  for any troubleshooting\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('punkt_tab', quiet=True)\n",
    "print(\"Startup completed.\")"
   ],
   "id": "ff813a05cf0e3bf3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Startup completed.\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Helper Functions\n",
    "The following functions are helper functions - Look below to see the functionality of each Function and/or look at the comments within the function.\n",
    "- open_json() : For opening a JSON file, such as the dataset or the corpus.\n",
    "- tokenize() : For splitting text into tokens, required for making a BM25 retriever. Uses the punk(_tab) tokenizer model by NLTK.\n",
    "- get_quest_corpus() : Corpus Loader for the QUEST Dataset Corpus - It is made in a special format, that cannot be easily stored with the simple open_json() function as we require the title separate from the text.\n",
    "- apply_exclusion() : Applies a boost to, penalty to or zeros out scores for documents containing one of the terms in the exclusion_criteria list.\n"
   ],
   "id": "896ac8dfd2e5da24"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-15T16:22:35.603548Z",
     "start_time": "2025-12-15T16:22:35.599262Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# JSON loader - For opening a JSON file, such as the dataset or the corpus\n",
    "def open_json(filename):\n",
    "    \"\"\"\n",
    "    Json File Opener\n",
    "\n",
    "    Args:\n",
    "        filename (string): the name of the json file to be opened\n",
    "    Returns:\n",
    "        dict: a dictionary of the json file\n",
    "    \"\"\"\n",
    "    with open(filename, 'r', encoding='utf-8') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "# Tokenizer by NLTK - For splitting text into Tokens - Uses punkt(_tab) tokenizer model\n",
    "def tokenize(text):\n",
    "    \"\"\"\n",
    "    Tokenizer by NLTK\n",
    "\n",
    "    Args:\n",
    "        text (str): document or query text to be tokenized\n",
    "    Returns:\n",
    "        list[tokens]: tokenized document or query text\n",
    "    \"\"\"\n",
    "    return nltk.word_tokenize(text.lower())\n",
    "\n",
    "# Corpus Loader for the Quest Dataset Corpus - It is made in a special format, that cannot be easily stored with the simple open_json() function\n",
    "def get_quest_corpus(filename):\n",
    "    \"\"\"\n",
    "    Corpus Loader for the Quest Dataset Corpus\n",
    "\n",
    "    Args:\n",
    "        filename (string): the name of the json file containing the QUEST corpus\n",
    "    Returns:\n",
    "        list[titles]: list of titles from QUEST corpus\n",
    "        list[documents]: list of documents from QUEST corpus\n",
    "    \"\"\"\n",
    "    titles = []\n",
    "    documents = []\n",
    "    print(\"Loading Quest Corpus...\")\n",
    "    with open(filename, \"r\") as f:\n",
    "            for line in tqdm(f):\n",
    "                doc  = json.loads(line)\n",
    "                titles.append(doc.get('title', ''))\n",
    "                documents.append(doc.get('text', ''))\n",
    "    return titles, documents\n",
    "\n",
    "# Applies a boost to, penalty to or zeros out scores for documents containing one of the terms in the exclusion_criteria list.\n",
    "def apply_exclusion(scores, documents, exclusion_criteria, mode, weight=0.5):\n",
    "    \"\"\"\n",
    "    Rescores scores based on whether documents include exclusion_criteria terms. Rescoring is based on weight (0.5 for this research) and a mode (check args).\n",
    "\n",
    "    Args:\n",
    "        scores (list): a list of scores\n",
    "        documents (list): a list of document text\n",
    "        exclusion_criteria (list): a list of exclusion_criteria terms\n",
    "        mode (string):\n",
    "            'boost' - Boost the score of documents including exclusion criteria\n",
    "            'penalize' - Penalize the score of documents including exclusion criteria\n",
    "            'filter' - 'Filter out' documents including exclusion criteria (zero their scores out)\n",
    "        weight (float): weight of exclusion mode impact boost/penalize\n",
    "            1.0 -- Doubles score for Boost, zero's score for Penalize - Extreme\n",
    "            0.5 -- Average weight impact - What we use in this Research!\n",
    "            0.0 -- No weight impact - boost/penalize don't do anything\n",
    "    Returns:\n",
    "        list[float]: a list of scores after applying exclusion\n",
    "    \"\"\"\n",
    "    # If exclusion criteria not provided, don't alter the scores\n",
    "    if not exclusion_criteria:\n",
    "        return scores\n",
    "    # Check all documents for exclusion_criteria terms and prepare list for applying weighted mode.\n",
    "    matches = np.array([\n",
    "        any(criteria.lower() in doc.lower() for criteria in exclusion_criteria) for doc in documents\n",
    "    ], dtype=bool)\n",
    "    if mode == 'boost':\n",
    "        scores[matches] *= (1 + weight)\n",
    "    elif mode == 'penalize':\n",
    "        scores[matches] *= (1 - weight)\n",
    "    elif mode == 'filter':\n",
    "        scores[matches] = 0.0\n",
    "    return scores"
   ],
   "id": "cf2cda5d86c4bf48",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Evaluation Function NEVIR\n",
    "\n",
    "In our Research this function is used to test the performance of the BM25 retriever on the NEVIR dataset with or without our applied strategy.\n",
    "1. Baseline\n",
    "2. Rewritten with LLM\n",
    "3. Rewritten with LLM, boosting documents containing exclusion criteria terms provided by LLM\n",
    "4. Rewritten with LLM, penalizing documents containing exclusion criteria terms provided by LLM\n",
    "5. Rewritten with LLM, filtering out (making score 0) documents containing exclusion criteria terms provided by LLM"
   ],
   "id": "33625eef92e58ec9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-10T18:18:50.952743Z",
     "start_time": "2025-12-10T18:18:50.949016Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def eval_nevir(data, baseline=True, exclusion=False, mode='boost', weight=0.5):\n",
    "    \"\"\"\n",
    "    Evaluation function for NevIr that can run on different modes:\n",
    "        - Baseline\n",
    "        - Rewritten\n",
    "        - Rewritten with Boost Exclusion\n",
    "        - Rewritten with Penalize Exclusion\n",
    "        - Rewritten with Filter Exclusion\n",
    "    Args:\n",
    "        data (dict): a dictionary of the json file containing the dataset\n",
    "        baseline (bool): if True, we are doing baseline testing, otherwise we are using rewritten queries\n",
    "        exclusion (bool): if True, we take the exclusion criteria and use the apply_exclusion function on the scores\n",
    "        mode (string): decides the exclusion mode\n",
    "        weight (float): decides the impact of the exclusion mode (except for filter - 0.5 for our Research)\n",
    "    Returns:\n",
    "        pairwise_accuracy (float): Pairwise accuracy of the entire dataset with the BM25 sparse retriever\n",
    "    \"\"\"\n",
    "    # Variables to calculate final pairwise_accuracy metric\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    print(f\"Evaluating NevIR | Rewritten: {not baseline} | Exclusion: {exclusion} | Mode: {mode} | Weight: {weight}\")\n",
    "    # Loop through all dataset samples\n",
    "    for sample in tqdm(data):\n",
    "        # Documents are always standard\n",
    "        docs = [sample['doc1'], sample['doc2']]\n",
    "        # If baseline, take original queries\n",
    "        if baseline:\n",
    "            q1 = sample['q1']\n",
    "            q2 = sample['q2']\n",
    "        # Otherwise take the rewritten querie\n",
    "        else:\n",
    "            q1 = sample['rewritten_query_q1']\n",
    "            q2 = sample['rewritten_query_q2']\n",
    "\n",
    "        # Tokenize the documents and initialize a bm25 instance\n",
    "        tokenized_docs = [tokenize(d) for d in docs]\n",
    "        bm25 = bm25s.BM25()\n",
    "        bm25.index(tokenized_docs, show_progress=False)\n",
    "\n",
    "        # Tokenize and then score each query\n",
    "        scores_q1 = bm25.get_scores(tokenize(q1))\n",
    "        scores_q2 = bm25.get_scores(tokenize(q2))\n",
    "\n",
    "        # On exclusion, we take the exclusion criteria's for both queries and apply the exclusion helper function to both scores\n",
    "        if exclusion and not baseline:\n",
    "            exclusion_q1 = sample['exclusion_criteria_q1']\n",
    "            exclusion_q2 = sample['exclusion_criteria_q2']\n",
    "            scores_q1 = apply_exclusion(scores_q1, docs, exclusion_q1, mode, weight)\n",
    "            scores_q2 = apply_exclusion(scores_q2, docs, exclusion_q2, mode, weight)\n",
    "\n",
    "        # Pairwise accuracy, so if both scores are correct we count it as a correct result for this entry\n",
    "        if (np.argmax(scores_q1) == 0) and (np.argmax(scores_q2) == 1):\n",
    "            correct += 1\n",
    "\n",
    "        # Total is always +1 for every entry\n",
    "        total += 1\n",
    "\n",
    "    pairwise_accuracy = correct / total\n",
    "    print(f\"Pairwise Accuracy for this run: {pairwise_accuracy * 100:.2f}%\")\n",
    "    return pairwise_accuracy"
   ],
   "id": "9489d81e56996fb4",
   "outputs": [],
   "execution_count": 103
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# NEVIR Testing\n",
    "\n",
    "- First, we load the NEVIR dataset.\n",
    "- Afterwards, we call the evaluate function for the 5 scenario's described above in this document.\n",
    "\n",
    "*Please note that weight=0.5 is used throughout all parts of our research*"
   ],
   "id": "749568b7e880bb88"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-10T18:18:34.794570Z",
     "start_time": "2025-12-10T18:18:34.757954Z"
    }
   },
   "cell_type": "code",
   "source": "nevir_dataset = open_json(\"NevIR_test_final.json\")",
   "id": "32e65ec1c65c6ada",
   "outputs": [],
   "execution_count": 101
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-10T18:18:41.300011Z",
     "start_time": "2025-12-10T18:18:35.523816Z"
    }
   },
   "cell_type": "code",
   "source": [
    "eval_nevir(nevir_dataset)\n",
    "eval_nevir(nevir_dataset, baseline=False)\n",
    "eval_nevir(nevir_dataset, baseline=False, exclusion=True, mode='boost', weight=0.5)\n",
    "eval_nevir(nevir_dataset, baseline=False, exclusion=True, mode='penalize', weight=0.5)\n",
    "eval_nevir(nevir_dataset, baseline=False, exclusion=True, mode='filter', weight=0.5)"
   ],
   "id": "782a4b651311f8b5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating NevIR | Rewritten: False | Exclusion: False | Mode: boost | Weight: 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1383/1383 [00:01<00:00, 1171.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pairwise Accuracy for this run: 4.70%\n",
      "Evaluating NevIR | Rewritten: True | Exclusion: False | Mode: boost | Weight: 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1383/1383 [00:01<00:00, 1210.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pairwise Accuracy for this run: 8.60%\n",
      "Evaluating NevIR | Rewritten: True | Exclusion: True | Mode: boost | Weight: 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1383/1383 [00:01<00:00, 1205.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pairwise Accuracy for this run: 8.60%\n",
      "Evaluating NevIR | Rewritten: True | Exclusion: True | Mode: penalize | Weight: 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1383/1383 [00:01<00:00, 1202.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pairwise Accuracy for this run: 8.75%\n",
      "Evaluating NevIR | Rewritten: True | Exclusion: True | Mode: filter | Weight: 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1383/1383 [00:01<00:00, 1206.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pairwise Accuracy for this run: 9.62%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.09616775126536514"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 102
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Evaluation Function QUEST\n",
    "\n",
    "In our Research this function is used to test the performance of the BM25 retriever on the QUEST dataset with or without our applied strategy.\n",
    "1. Baseline\n",
    "2. Rewritten with LLM\n",
    "3. Rewritten with LLM, boosting documents containing exclusion criteria terms provided by LLM\n",
    "4. Rewritten with LLM, penalizing documents containing exclusion criteria terms provided by LLM\n",
    "5. Rewritten with LLM, filtering out (making score 0) documents containing exclusion criteria terms provided by LLM"
   ],
   "id": "357b10aa066f9e71"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-10T18:29:21.178558Z",
     "start_time": "2025-12-10T18:29:21.172311Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def eval_quest(data, retriever, documents, titles, baseline=True, exclusion=False, mode='penalize', weight=0.5, top_k=100):\n",
    "    \"\"\"\n",
    "    Evaluation function for QUEST that can run on different modes:\n",
    "        - Baseline\n",
    "        - Rewritten\n",
    "        - Rewritten with Boost Exclusion\n",
    "        - Rewritten with Penalize Exclusion\n",
    "        - Rewritten with Filter Exclusion\n",
    "    Args:\n",
    "        data (dict): a dictionary of the json file containing the dataset\n",
    "        baseline (bool): if True, we are doing baseline testing, otherwise we are using rewritten queries\n",
    "        exclusion (bool): if True, we take the exclusion criteria and use the apply_exclusion function on the scores\n",
    "        mode (string): decides the exclusion mode\n",
    "        weight (float): decides the impact of the exclusion mode (except for filter - 0.5 for our Research)\n",
    "        top_k (int): decides the number of highest-scoring documents to take into account when calculating the final metrics\n",
    "    Returns:\n",
    "        avg_f1 (float): Average F1 score over all queries in the dataset\n",
    "        avg_prec (float): Average precision score over all queries in the dataset\n",
    "        avg_rec (float): Average recall score over all queries in the dataset\n",
    "    \"\"\"\n",
    "    # Variables to calculate final metrics (taken from QUEST paper)\n",
    "    f1_list = []\n",
    "    prec_list = []\n",
    "    rec_list = []\n",
    "    print(f\"Evaluating QUEST | Rewritten: {not baseline} | Exclusion: {exclusion} | Mode: {mode} | Weight: {weight}\")\n",
    "\n",
    "    # If baseline, take original queries\n",
    "    if baseline:\n",
    "        queries = [sample.get('original_query_cleaned') for sample in data]\n",
    "    # Otherwise take the rewritten queries\n",
    "    else:\n",
    "        queries = [sample.get('rewritten_query') for sample in data]\n",
    "\n",
    "    query_tokens = [tokenize(query) for query in tqdm(queries, desc='Tokenizing Queries')]\n",
    "    # Take a larger initial top_k selection to make sure we can apply scoring boost/penalties and after that take the actual top_k\n",
    "    initial_k = top_k * 10 if (exclusion and not baseline) else top_k\n",
    "    # Get indices and scores for all queries\n",
    "    results_indices, results_scores = retriever.retrieve(query_tokens, k=initial_k)\n",
    "\n",
    "    # Loop through all dataset samples\n",
    "    for i, sample in tqdm(enumerate(data), total=len(data), desc=\"Calculating Metrics\"):\n",
    "        # Get the indices and scores for this sample\n",
    "        top_indices = results_indices[i]\n",
    "        top_scores = results_scores[i]\n",
    "        # On exclusion, we take the exclusion criteria's\n",
    "        if exclusion and not baseline:\n",
    "            excl_criteria = sample.get('exclusion_criteria', [])\n",
    "            if excl_criteria and documents:\n",
    "                # Take the documents contained in the top_k*100 indices, and use the helper function to get new scores\n",
    "                local_docs = [documents[idx] for idx in top_indices]\n",
    "                local_scores = top_scores.astype(float)\n",
    "                local_scores = apply_exclusion(local_scores, local_docs, excl_criteria, mode, weight)\n",
    "                # Sort new scores and take the top_k (Standard 100)\n",
    "                sorted_local_scores = np.argsort(local_scores)[::-1]\n",
    "                top_indices = top_indices[sorted_local_scores][:top_k]\n",
    "        # Everything below this is 'indirectly' copied from the QUEST dataset code\n",
    "        # https://github.com/google-research/language/blob/master/language/quest/eval/run_eval.py\n",
    "        # The gold_titles in our case are the titles of documents that are relevant for our query\n",
    "        gold_titles = set(sample['docs'])\n",
    "        # The predicted titles in our case are the titles (top_k) predicted to be relevant for our query by the bm25 retriever\n",
    "        retrieved_titles = [titles[idx] for idx in top_indices]\n",
    "        predicted_titles = set(retrieved_titles)\n",
    "        # Calculates the metrics\n",
    "        tp = len(gold_titles.intersection(predicted_titles))\n",
    "        fp = len(predicted_titles.difference(gold_titles))\n",
    "        fn = len(gold_titles.difference(predicted_titles))\n",
    "        if tp:\n",
    "            prec = tp / (tp + fp)\n",
    "            rec = tp / (tp + fn)\n",
    "            f1 = 2 * prec * rec / (prec + rec)\n",
    "        else:\n",
    "            prec = 0.0\n",
    "            rec = 0.0\n",
    "            f1 = 0.0\n",
    "\n",
    "        f1_list.append(f1)\n",
    "        prec_list.append(prec)\n",
    "        rec_list.append(rec)\n",
    "    # Take the mean over all queries\n",
    "    avg_f1 = np.mean(f1_list) if f1_list else 0.0\n",
    "    avg_prec = np.mean(prec_list) if prec_list else 0.0\n",
    "    avg_rec = np.mean(rec_list) if rec_list else 0.0\n",
    "\n",
    "    print(f\"Quest Results --> F1: {avg_f1:.4f} | PREC: {avg_prec:.4f} | REC: {avg_rec:.4f}\")\n",
    "    print(\"----------------------------------------------------------------------------------------------------------------------------\")\n",
    "    return avg_f1, avg_prec, avg_rec"
   ],
   "id": "70f183861c47f5ce",
   "outputs": [],
   "execution_count": 104
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# QUEST Testing\n",
    "\n",
    "- First, we load the QUEST dataset.\n",
    "- Then we use the helper to load the corpus titles and documents.\n",
    "- Then we initialize a BM25 instance on the corpus documents tokenized using NLTK tokenizer.\n",
    "- Afterwards, we call the evaluate function for the 5 scenario's described above in this document.\n",
    "\n",
    "*Please note that weight=0.5 is used throughout all parts of our research*<br>\n",
    "*Also note that k=100 is chosen from the original QUEST research paper*"
   ],
   "id": "73ba9ecab0c054d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-10T18:36:02.921399Z",
     "start_time": "2025-12-10T18:29:27.964665Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load the dataset that includes rewritten queries\n",
    "quest_dataset = open_json(\"test_negations_final.json\")\n",
    "# Load the corpus titles and documents into lists\n",
    "quest_titles, quest_documents = get_quest_corpus(\"documents.jsonl\")\n",
    "# Initialize a bm25 instance on the tokenized corpus documents\n",
    "quest_bm25 = bm25s.BM25()\n",
    "quest_tokenized_docs = [tokenize(doc) for doc in tqdm(quest_documents)]\n",
    "quest_bm25.index(quest_tokenized_docs)"
   ],
   "id": "ba10defe023d12f6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Quest Corpus...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "325505it [00:02, 117244.23it/s]\n",
      "100%|██████████| 325505/325505 [05:37<00:00, 965.26it/s] \n",
      "                                                                                           \r"
     ]
    }
   ],
   "execution_count": 105
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-10T18:36:19.684796Z",
     "start_time": "2025-12-10T18:36:02.936321Z"
    }
   },
   "cell_type": "code",
   "source": [
    "eval_quest(quest_dataset, quest_bm25, quest_documents, quest_titles)\n",
    "eval_quest(quest_dataset, quest_bm25, quest_documents, quest_titles, False)\n",
    "eval_quest(quest_dataset, quest_bm25, quest_documents, quest_titles, False, True, mode='boost', weight=0.5, top_k=100)\n",
    "eval_quest(quest_dataset, quest_bm25, quest_documents, quest_titles, False, True, mode='penalize', weight=0.5, top_k=100)\n",
    "eval_quest(quest_dataset, quest_bm25, quest_documents, quest_titles, False, True, mode='filter', weight=0.5, top_k=100)"
   ],
   "id": "19ef62dd76411b93",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating QUEST | Rewritten: False | Exclusion: False | Mode: penalize | Weight: 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing Queries: 100%|██████████| 207/207 [00:00<00:00, 38987.87it/s]\n",
      "Calculating Metrics: 100%|██████████| 207/207 [00:00<00:00, 46819.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quest Results --> F1: 0.0254 | PREC: 0.0143 | REC: 0.1180\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "Evaluating QUEST | Rewritten: True | Exclusion: False | Mode: penalize | Weight: 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing Queries: 100%|██████████| 207/207 [00:00<00:00, 61922.90it/s]\n",
      "Calculating Metrics: 100%|██████████| 207/207 [00:00<00:00, 44058.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quest Results --> F1: 0.0366 | PREC: 0.0207 | REC: 0.1677\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "Evaluating QUEST | Rewritten: True | Exclusion: True | Mode: boost | Weight: 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing Queries: 100%|██████████| 207/207 [00:00<00:00, 61280.42it/s]\n",
      "Calculating Metrics: 100%|██████████| 207/207 [00:02<00:00, 70.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quest Results --> F1: 0.0313 | PREC: 0.0177 | REC: 0.1717\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "Evaluating QUEST | Rewritten: True | Exclusion: True | Mode: penalize | Weight: 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing Queries: 100%|██████████| 207/207 [00:00<00:00, 53729.87it/s]\n",
      "Calculating Metrics: 100%|██████████| 207/207 [00:02<00:00, 71.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quest Results --> F1: 0.0348 | PREC: 0.0197 | REC: 0.1854\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "Evaluating QUEST | Rewritten: True | Exclusion: True | Mode: filter | Weight: 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing Queries: 100%|██████████| 207/207 [00:00<00:00, 60659.61it/s]\n",
      "Calculating Metrics: 100%|██████████| 207/207 [00:02<00:00, 72.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quest Results --> F1: 0.0343 | PREC: 0.0194 | REC: 0.1834\n",
      "----------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(np.float64(0.03432507627850995),\n",
       " np.float64(0.019414575313126035),\n",
       " np.float64(0.1834254767814691))"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 106
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Evaluation Function EXCLUIR\n",
    "\n",
    "In our Research this function is used to test the performance of the BM25 retriever on the EXCLUIR dataset with or without our applied strategy.\n",
    "1. Baseline\n",
    "2. Rewritten with LLM\n",
    "3. Rewritten with LLM, boosting documents containing exclusion criteria terms provided by LLM\n",
    "4. Rewritten with LLM, penalizing documents containing exclusion criteria terms provided by LLM\n",
    "5. Rewritten with LLM, filtering out (making score 0) documents containing exclusion criteria terms provided by LLM"
   ],
   "id": "63f3a3781227c441"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-15T16:22:47.084396Z",
     "start_time": "2025-12-15T16:22:47.078559Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def eval_excl(data, retriever, corpus_list, baseline=True, exclusion=False, mode='penalize', weight=0.5):\n",
    "    \"\"\"\n",
    "    Evaluation function for EXCLUIR that can run on different modes:\n",
    "        - Baseline\n",
    "        - Rewritten\n",
    "        - Rewritten with Boost Exclusion\n",
    "        - Rewritten with Penalize Exclusion\n",
    "        - Rewritten with Filter Exclusion\n",
    "    Args:\n",
    "        data (dict): a dictionary of the json file containing the dataset\n",
    "        baseline (bool): if True, we are doing baseline testing, otherwise we are using rewritten queries\n",
    "        exclusion (bool): if True, we take the exclusion criteria and use the apply_exclusion function on the scores\n",
    "        mode (string): decides the exclusion mode\n",
    "        weight (float): decides the impact of the exclusion mode (except for filter - 0.5 for our Research)\n",
    "    Returns:\n",
    "        avg_r1 (float): average R@1 score  (Recall @ 1)\n",
    "        avg_mrr (float): average MRR@10 score (MRR @ 10)\n",
    "        avg_rr (float): average RR score (RightRank over top half of highest scores)\n",
    "    \"\"\"\n",
    "    # Variables to calculate final metrics (taken from EXCLUIR paper)\n",
    "    rec1_list = []\n",
    "    mrr10_list = []\n",
    "    rr_list = []\n",
    "    print(f\"Evaluating EXCLUIR | Rewritten: {not baseline} | Exclusion: {exclusion} | Mode: {mode} | Weight: {weight}\")\n",
    "    # If baseline, take original queries\n",
    "    if baseline:\n",
    "        queries = [sample.get('RQ_rewrite') for sample in data]\n",
    "    # Otherwise take the rewritten queries\n",
    "    else:\n",
    "        queries = [sample.get('rewritten_query') for sample in data]\n",
    "\n",
    "    query_tokens = [tokenize(query) for query in tqdm(queries, desc='Tokenizing Queries')]\n",
    "    # Take a larg initial top_k selection to make sure we can apply scoring boost/penalties and still accurately represent @10 or @100 etc. metrics\n",
    "    initial_k = round(len(corpus_list))\n",
    "    # Get indices and scores for all queries\n",
    "    results_indices, results_scores = retriever.retrieve(query_tokens, k=initial_k)\n",
    "\n",
    "    # Loop through all dataset samples\n",
    "    for i, sample in tqdm(enumerate(data), total=len(data), desc=\"Calculating metrics\"):\n",
    "        # Get the indices and scores for this sample\n",
    "        top_indices = results_indices[i]\n",
    "        top_scores = results_scores[i]\n",
    "        # On exclusion, we take the exclusion criteria's\n",
    "        if exclusion and not baseline:\n",
    "            excl_criteria = sample.get('exclusion_criteria', [])\n",
    "            if excl_criteria and corpus_list:\n",
    "            \t# Take the documents contained in the candidate indices (half of corpus), and use the helper function to get new scores\n",
    "                local_docs = [corpus_list[idx] for idx in top_indices]\n",
    "                local_scores = top_scores.astype(float)\n",
    "                local_scores = apply_exclusion(local_scores, local_docs, excl_criteria, mode, weight)\n",
    "                # Sort new scores and take their indices\n",
    "                sorted_local_scores = np.argsort(local_scores)[::-1]\n",
    "                top_indices = top_indices[sorted_local_scores]\n",
    "\n",
    "        # Take the negative document index and positive document index from the EXCLUIR data sample (bad vs good document)\n",
    "        neg_idx = int(sample['corpus_sub_index'][0])\n",
    "        pos_idx = int(sample['corpus_sub_index'][1])\n",
    "        # Find if and where they are in the top_indices (if not they are on index 'infinite)\n",
    "        try:\n",
    "            neg_rank = np.where(top_indices == neg_idx)[0][0]\n",
    "        except IndexError:\n",
    "            neg_rank = float('inf')\n",
    "        try:\n",
    "            pos_rank = np.where(top_indices == pos_idx)[0][0]\n",
    "        except IndexError:\n",
    "            pos_rank = float('inf')\n",
    "\n",
    "        # If good document in top1 spot, recall for this query is 1, otherwise 0\n",
    "        rec1 = 1.0 if pos_rank == 0 else 0.0\n",
    "        rec1_list.append(rec1)\n",
    "\n",
    "        # If good document in top10 spot, mrr is 1/(rank+1), if outside spot 0\n",
    "        mrr = 0.0\n",
    "        if pos_rank < 10:\n",
    "            mrr = 1.0 / (pos_rank + 1)\n",
    "        mrr10_list.append(mrr)\n",
    "\n",
    "        # If good document ranked better than bad document RR is 0, otherwise 0\n",
    "        rr_val = 1.0 if pos_rank < neg_rank else 0.0\n",
    "        rr_list.append(rr_val)\n",
    "    # Take the mean over all queries\n",
    "    avg_r1 = np.mean(rec1_list)\n",
    "    avg_mrr = np.mean(mrr10_list)\n",
    "    avg_rr = np.mean(rr_list)\n",
    "\n",
    "    print(f\"EXCLUIR Results --> R@1: {avg_r1:.4f} | MRR@10: {avg_mrr:.4f} | RightRank: {avg_rr:.4f}\")\n",
    "    print(\"----------------------------------------------------------------------------------------------------------------------------\")\n",
    "    return avg_r1, avg_mrr, avg_rr\n"
   ],
   "id": "92317b0ca28285e4",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# EXCLUIR Testing\n",
    "\n",
    "- First, we load the EXCLUIR dataset.\n",
    "- Then we use the helper function to load the documents from the corpus.\n",
    "- Then we initialize a BM25 instance on the corpus documents tokenized using NLTK tokenizer.\n",
    "- Afterwards, we call the evaluate function for the 5 scenario's described above in this document.\n",
    "\n",
    "*Please note that weight=0.5 is used throughout all parts of our research*"
   ],
   "id": "db5fbc8297bff065"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-15T16:23:09.115454Z",
     "start_time": "2025-12-15T16:22:49.820449Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load the dataset that includes rewritten queries\n",
    "excluir_data = open_json('ExcluIR_test_manual_final.json')\n",
    "# Load the corpus that includes documents\n",
    "excluir_documents = open_json('corpus.json')\n",
    "# Initialize a bm25 instance on the tokenized corpus documents\n",
    "excluir_bm25 = bm25s.BM25()\n",
    "excluir_tokenized = [tokenize(doc) for doc in tqdm(excluir_documents)]\n",
    "excluir_bm25.index(excluir_tokenized)"
   ],
   "id": "d83c12042ba2f5a8",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 90406/90406 [00:15<00:00, 5869.96it/s]\n",
      "                                                                                          \r"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-15T16:50:23.679887Z",
     "start_time": "2025-12-15T16:23:11.318337Z"
    }
   },
   "cell_type": "code",
   "source": [
    "eval_excl(excluir_data, excluir_bm25, excluir_documents)\n",
    "eval_excl(excluir_data, excluir_bm25, excluir_documents, False)\n",
    "eval_excl(excluir_data, excluir_bm25, excluir_documents, False, True, mode='boost', weight=0.5)\n",
    "eval_excl(excluir_data, excluir_bm25, excluir_documents, False, True, mode='penalize', weight=0.5)\n",
    "eval_excl(excluir_data, excluir_bm25, excluir_documents, False, True, mode='filter', weight=0.5)"
   ],
   "id": "3936b578cab3e65",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating EXCLUIR | Rewritten: False | Exclusion: False | Mode: penalize | Weight: 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing Queries: 100%|██████████| 3452/3452 [00:00<00:00, 21036.33it/s]\n",
      "Calculating metrics: 100%|██████████| 3452/3452 [00:00<00:00, 14341.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXCLUIR Results --> R@1: 0.4983 | MRR@10: 0.6529 | RightRank: 0.5397\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "Evaluating EXCLUIR | Rewritten: True | Exclusion: False | Mode: penalize | Weight: 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing Queries: 100%|██████████| 3452/3452 [00:00<00:00, 27145.41it/s]\n",
      "Calculating metrics: 100%|██████████| 3452/3452 [00:00<00:00, 16227.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXCLUIR Results --> R@1: 0.6347 | MRR@10: 0.7167 | RightRank: 0.8441\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "Evaluating EXCLUIR | Rewritten: True | Exclusion: True | Mode: boost | Weight: 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing Queries: 100%|██████████| 3452/3452 [00:00<00:00, 33257.47it/s]\n",
      "Calculating metrics: 100%|██████████| 3452/3452 [08:11<00:00,  7.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXCLUIR Results --> R@1: 0.6188 | MRR@10: 0.7182 | RightRank: 0.7590\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "Evaluating EXCLUIR | Rewritten: True | Exclusion: True | Mode: penalize | Weight: 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing Queries: 100%|██████████| 3452/3452 [00:00<00:00, 31150.47it/s]\n",
      "Calculating metrics: 100%|██████████| 3452/3452 [08:19<00:00,  6.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXCLUIR Results --> R@1: 0.4893 | MRR@10: 0.5634 | RightRank: 0.8943\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "Evaluating EXCLUIR | Rewritten: True | Exclusion: True | Mode: filter | Weight: 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing Queries: 100%|██████████| 3452/3452 [00:00<00:00, 24443.78it/s]\n",
      "Calculating metrics: 100%|██████████| 3452/3452 [07:41<00:00,  7.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXCLUIR Results --> R@1: 0.4389 | MRR@10: 0.4946 | RightRank: 0.7784\n",
      "----------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(np.float64(0.4388760139049826),\n",
       " np.float64(0.4946250390847725),\n",
       " np.float64(0.7783893395133256))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "6d64c1f8aedf03ff"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
