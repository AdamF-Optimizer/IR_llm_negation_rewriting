{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-14T14:26:38.689667Z",
     "start_time": "2025-11-14T14:26:30.259166Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Imports\n",
    "from rank_bm25 import BM25Okapi\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "import json\n",
    "import nltk\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')"
   ],
   "id": "ff813a05cf0e3bf3",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Timov\\PyCharmMiscProject\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Timov\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Timov\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-14T14:26:43.300443Z",
     "start_time": "2025-11-14T14:26:43.298104Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Tokenizer\n",
    "def tokenize(text):\n",
    "    return nltk.word_tokenize(text.lower())\n",
    "\n",
    "# JSON loader\n",
    "def open_json(filename):\n",
    "    with open(filename, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    return data"
   ],
   "id": "cf2cda5d86c4bf48",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Loading Dataset NevIR",
   "id": "33625eef92e58ec9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-14T14:31:09.134588Z",
     "start_time": "2025-11-14T14:31:09.130855Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Function for the NevIR Dataset (Exclusion yet to be done)\n",
    "\n",
    "def nevir(data, data2=None):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    # No rewritten dataset provided\n",
    "    if data2 is None:\n",
    "        print(\"Evaluating NevIR original Dataset.\")\n",
    "    # Rewritten Dataset provided, make a loopup set for those queries\n",
    "    else:\n",
    "        print(\"Evaluating NevIR queries rewritten.\")\n",
    "        data2_lookup = {item['id']: item for item in data2}\n",
    "    for i, sample in enumerate(tqdm(data, desc=\"Evaluating BM25 performance on NevIR\")):\n",
    "        # Take the original documents (never rewritten)\n",
    "        docs = [sample['doc1'], sample['doc2']]\n",
    "        # If no rewritten queries, take original queries\n",
    "        if data2 is None:\n",
    "            q1 = sample['q1']\n",
    "            q2 = sample['q2']\n",
    "        # Otherwise take the rewritten queries that match the id on the documents (to have correct pairs)\n",
    "        else:\n",
    "            sid = sample['id']\n",
    "            if sid not in data2_lookup:\n",
    "                print(f\"ID {sid} not found in data2, skipping.\")\n",
    "                continue\n",
    "            q1 = data2_lookup[sid]['rewritten_query_q1']\n",
    "            q2 = data2_lookup[sid]['rewritten_query_q2']\n",
    "        # Tokenize the documents and initialize a bm25 instance\n",
    "        tokenized_docs = [tokenize(d) for d in docs]\n",
    "        bm25 = BM25Okapi(tokenized_docs)\n",
    "\n",
    "        # Score each query\n",
    "        scores_q1 = bm25.get_scores(tokenize(q1))\n",
    "        scores_q2 = bm25.get_scores(tokenize(q2))\n",
    "        # q1 should match doc1, q2 should match doc2\n",
    "        correct += int(np.argmax(scores_q1) == 0)\n",
    "        correct += int(np.argmax(scores_q2) == 1)\n",
    "        total += 2\n",
    "\n",
    "    accuracy = correct / total\n",
    "    print(f\"BM25 Pairwise Accuracy on NevIR: {accuracy * 100:.2f}%\")\n",
    "    return accuracy"
   ],
   "id": "9489d81e56996fb4",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-14T14:31:14.488162Z",
     "start_time": "2025-11-14T14:31:11.083146Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# NEVIR test set\n",
    "ds_nevir = load_dataset(\"orionweller/NevIR\")\n",
    "data_nevir = ds_nevir[\"test\"]\n",
    "\n",
    "# Run NevIR standard and with the rewritten dataset\n",
    "nevir(data_nevir)\n",
    "data2_nevir = open_json(\"NevIR_test_final.json\")\n",
    "nevir(data_nevir, data2_nevir)"
   ],
   "id": "782a4b651311f8b5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating NevIR original Dataset.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating BM25 performance on NevIR: 100%|██████████| 1383/1383 [00:01<00:00, 1272.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BM25 Pairwise Accuracy on NevIR: 48.16%\n",
      "Evaluating NevIR queries rewritten.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating BM25 performance on NevIR: 100%|██████████| 1383/1383 [00:01<00:00, 1314.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BM25 Pairwise Accuracy on NevIR: 48.81%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.4880694143167028"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-14T14:42:40.023920Z",
     "start_time": "2025-11-14T14:42:40.021405Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Get the corpus and load it into a dictionary based on title (yet to decide if this is the best)\n",
    "def get_corpus(filename):\n",
    "    corpus = {}\n",
    "    with open(filename, \"r\") as f:\n",
    "        total_docs = sum(1 for _ in f)\n",
    "    with open(filename, \"r\") as f:\n",
    "        for line in tqdm(f, total=total_docs):\n",
    "            d = json.loads(line)\n",
    "            corpus[d['title']] = {\"text\": d['text'], \"tokens\": tokenize(d['text'])}\n",
    "    return corpus"
   ],
   "id": "5998d3ff4b3c7657",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-14T14:42:40.004389Z",
     "start_time": "2025-11-14T14:36:46.663620Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load the dataset that includes rewritten queries, and get the corpus\n",
    "data_quest = open_json(\"test_negations_final.json\")\n",
    "corpus = get_corpus(\"documents.jsonl\")"
   ],
   "id": "1e9e822daf676406",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 325505/325505 [05:52<00:00, 924.11it/s] \n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-14T14:44:58.616623Z",
     "start_time": "2025-11-14T14:44:33.481200Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# We do this beforehand because it takes quite some time\n",
    "tokenized_corpus = [doc[\"tokens\"] for doc in corpus.values()]\n",
    "bm25 = BM25Okapi(tokenized_corpus)"
   ],
   "id": "51e158b49a9a7c96",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-14T16:10:46.182167Z",
     "start_time": "2025-11-14T16:10:46.178016Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Function to determine which docs are relevant\n",
    "def get_relevance(title, relevance_dict):\n",
    "    ratings = relevance_dict.get(title, {})\n",
    "    for r in ratings:\n",
    "        if r == \"Definitely relevant\" or r == \"Likely relevant\":\n",
    "            return True\n",
    "    return False"
   ],
   "id": "bb0d95808511f040",
   "outputs": [],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-14T14:57:14.193449Z",
     "start_time": "2025-11-14T14:57:14.190554Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Fucntion for the Quest Dataset (Exclusion yet to be done)\n",
    "\n",
    "def quest(data, bm25, corpus, baseline=True, exclusion=False):\n",
    "    total_rr = 0\n",
    "    correct_at_1 = 0\n",
    "    nr_of_queries = len(data)\n",
    "    titles = list(corpus.keys())\n",
    "    for sample in tqdm(data, desc=\"Evaluating BM25 on Quest Dataset\"):\n",
    "        # If baseline use the original query\n",
    "        if baseline:\n",
    "            query = sample['original_query_cleaned']\n",
    "        # Otherwise use the rewritten query\n",
    "        else:\n",
    "            query = sample['rewritten_query']\n",
    "        # Get the relevance ratings (Giving relevant titles for the query)\n",
    "        relevance_ratings = sample.get('metadata', {}).get('relevance_ratings') or {}\n",
    "\n",
    "        # Tokenize the query and get bm25 scores, sort them from high to low take top_k (100)\n",
    "        scores = bm25.get_scores(tokenize(query))\n",
    "        top_k = 100\n",
    "        ranked_indices = np.argsort(scores)[-top_k:][::-1]\n",
    "\n",
    "        # Calculate Reciprocal Rank, to calculate MRR later on\n",
    "        rr = 0\n",
    "        for rank, idx in enumerate(ranked_indices, start=1):\n",
    "            if get_relevance(titles[idx], relevance_ratings):\n",
    "                rr = 1.0 / rank\n",
    "                break\n",
    "        total_rr += rr\n",
    "        # Check acc for the best ranked indice, to get accuracy@1\n",
    "        if get_relevance(titles[ranked_indices[0]], relevance_ratings):\n",
    "            correct_at_1 += 1\n",
    "\n",
    "    # Final metrics\n",
    "    mrr = total_rr / nr_of_queries\n",
    "    acc_at_1 = correct_at_1 / nr_of_queries\n",
    "\n",
    "    print(f\"Mean Reciprocal Rank (MRR): {mrr:.4f}\")\n",
    "    print(f\"Accuracy@1: {acc_at_1:.4f}\")"
   ],
   "id": "70f183861c47f5ce",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-14T15:04:26.098762Z",
     "start_time": "2025-11-14T14:57:15.831199Z"
    }
   },
   "cell_type": "code",
   "source": [
    "quest(data_quest, bm25, corpus)\n",
    "quest(data_quest, bm25, corpus, False)"
   ],
   "id": "19ef62dd76411b93",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating BM25 on Quest Dataset: 100%|██████████| 207/207 [05:01<00:00,  1.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Reciprocal Rank (MRR): 0.0875\n",
      "Accuracy@1: 0.0338\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating BM25 on Quest Dataset: 100%|██████████| 207/207 [02:08<00:00,  1.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Reciprocal Rank (MRR): 0.1389\n",
      "Accuracy@1: 0.0725\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-14T15:04:54.363589Z",
     "start_time": "2025-11-14T15:04:54.074808Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load ExcluIR rewritten Dataset\n",
    "with open('ExcluIR_test_manual_final.json', 'r', encoding='utf-8') as f:\n",
    "    data_exclu = json.load(f)\n",
    "\n",
    "# Load the corpus (stored as Json with lists)\n",
    "with open('corpus.json', 'r', encoding='utf-8') as f:\n",
    "    corpus_exclu = json.load(f)"
   ],
   "id": "548c3df7417ae06d",
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-14T15:10:34.659081Z",
     "start_time": "2025-11-14T15:10:34.655952Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Fucntion for the ExcluIR Dataset (Exclusion yet to be done)\n",
    "# Corpus will be used later for the exclusion check\n",
    "\n",
    "def excl(data, corpus, bm25, baseline=True, exclusion=False):\n",
    "    total_rr = 0\n",
    "    correct_at_1 = 0\n",
    "    nr_of_queries = len(data)\n",
    "    for sample in tqdm(data, desc=\"Evaluating BM25 on ExcluIR Dataset\"):\n",
    "        # If baseline use the original query\n",
    "        if baseline:\n",
    "            query = sample['question0']\n",
    "        # Otherwise use the rewritten query\n",
    "        else:\n",
    "            query = sample['rewritten_query']\n",
    "        # Get the relevance ratings (Giving relevant document indices for the query)\n",
    "        relevant_indices = set(sample['corpus_sub_index'])\n",
    "\n",
    "        # Tokenize the query and get bm25 scores, sort them from high to low take top_k (100)\n",
    "        scores = bm25.get_scores(tokenize(query))\n",
    "        top_k = 100\n",
    "        ranked_indices = np.argsort(scores)[-top_k:][::-1]\n",
    "\n",
    "        # Calculate Reciprocal Rank, to calculate MRR later on\n",
    "        rr = 0\n",
    "        for rank, idx in enumerate(ranked_indices, start=1):\n",
    "            if idx in relevant_indices:\n",
    "                rr = 1.0 / rank\n",
    "                break\n",
    "        total_rr += rr\n",
    "        # Check acc for the best ranked indice, to get accuracy@1\n",
    "        if ranked_indices[0] in relevant_indices:\n",
    "            correct_at_1 += 1\n",
    "\n",
    "    # Final metrics\n",
    "    mrr = total_rr / nr_of_queries\n",
    "    acc_at_1 = correct_at_1 / nr_of_queries\n",
    "\n",
    "    print(f\"Baseline BM25 MRR: {mrr:.4f}\")\n",
    "    print(f\"Baseline BM25 Acc@1: {acc_at_1:.4f}\")\n"
   ],
   "id": "92317b0ca28285e4",
   "outputs": [],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-14T15:05:23.938971Z",
     "start_time": "2025-11-14T15:05:09.065153Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# We do this beforehand because it takes quite some time\n",
    "tokenized_corpus_exclu = [tokenize(doc) for doc in corpus_exclu]\n",
    "bm25_exclu = BM25Okapi(tokenized_corpus_exclu)"
   ],
   "id": "7fc0ddf4c1b605d0",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-14T15:55:48.028370Z",
     "start_time": "2025-11-14T15:10:41.900100Z"
    }
   },
   "cell_type": "code",
   "source": [
    "excl(data_exclu, corpus_exclu, bm25_exclu)\n",
    "excl(data_exclu, corpus_exclu, bm25_exclu, False)"
   ],
   "id": "3936b578cab3e65",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating BM25 on ExcluIR Dataset: 100%|██████████| 3452/3452 [26:21<00:00,  2.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline BM25 MRR: 0.8996\n",
      "Baseline BM25 Acc@1: 0.8627\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating BM25 on ExcluIR Dataset: 100%|██████████| 3452/3452 [18:44<00:00,  3.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline BM25 MRR: 0.7914\n",
      "Baseline BM25 Acc@1: 0.7335\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 28
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
